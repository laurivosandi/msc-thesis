\RequirePackage{ifpdf}
\documentclass[a4paper,11pt]{kth-mag}
\let\newfloat\undefined
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[english]{babel}
\usepackage{modifications}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{pgfplots}

\usepackage{listings}
\usepackage[numbers, square]{natbib}
\bibliographystyle{IEEEtranN}
\usepackage{color}
\usepackage[pdfusetitle,pdftex,colorlinks]{hyperref}
\hypersetup{pdfborder={0 0 0}}
\hypersetup{bookmarksdepth=3}
\hypersetup{bookmarksopen=true}
\hypersetup{bookmarksopenlevel=1}
\hypersetup{bookmarksnumbered=true}
\hypersetup{colorlinks=false}
\usepackage[toc,acronym]{glossaries}


%% Listings
\usepackage{listings}
\lstset{escapechar=\%, frame=tb, basicstyle=\ttfamily, language=Java}
\lstset{prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\rhookswarrow}}}
\lstset{postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\rcurvearrowse\space}}}
\lstset{breaklines=true, breakatwhitespace=false}
\lstset{numbers=left, numberstyle=\scriptsize, firstnumber=1}

%% Other
\usepackage{enumerate}
\usepackage{paralist}

\usepackage{relsize}


%% COMMENTS %%
\newcommand{\TODO}[1]{\begingroup\def\thefootnote{\textcolor{red}{TODO}}\footnote{\textcolor{red}{#1}}\endgroup}
\newcommand{\TODOP}[1]{\par\textcolor{red}{#1}\marginpar{\textcolor{red}{TODO}}}
\newcommand{\TODOX}[1]{\textcolor{red}{#1}\marginpar{\textcolor{red}{TODO}}}

\hyphenation{Butterknife}

\title{Efficient and Reliable Filesystem Snapshot Distribution}

\subtitle{}
\author{Lauri Võsandi}
\date{}
\blurb{Master of Science Thesis in\\ Information and Communication Technology\\ Supervisor: Lars Kroll \\Examiner: Dr. Jim Dowling \\ Stockholm, Sweden, June 2015}
\trita{TRITA-ICT-EX-2015:???}

\makeglossaries

\begin{document}
\newacronym{acid}{ACID}{atomicity, consistency, isolation and durability properties}
\newacronym{api}{API}{application programming interface}
\newacronym{dht}{DHT}{distributed hash table}
\newacronym{dsl}{DSL}{domain specific language}
\newacronym{fai}{FAI}{Fully Automated Installation}
\newacronym{lxc}{LXC}{Linux Containers}
\newacronym{hdd}{HDD}{hard disk drive}
\newacronym{nic}{NIC}{network interface card}
\newacronym{p2p}{P2P}{peer-to-peer}
\newacronym{pxe}{PXE}{Preboot eXecution Environment}
\newacronym{s3}{S3}{simple storage service}
\newacronym{sics}{SICS}{Swedish Institute of Computer Science}
\newacronym{sla}{SLA}{service level agreement}
\newacronym{ssd}{SSD}{solid state disk}
\newacronym{tcp}{TCP}{transmission control protocol}
\newacronym{grub}{GRUB}{GRand Unified Bootloader}


\frontmatter
\pagestyle{empty}
\removepagenumbers
\maketitle
\selectlanguage{english}

%
%
% ABSTRACT
%
%
\begin{abstract}

Software deployment and updates 

TODO

\end{abstract}

\chapter*{Acknowledgements}

I would like to thank my examiner \emph{Dr. Jim Dowling} and my supervisor \emph{Lars Kroll} firstly for providing me with the opportunity to carry out this research and secondly for guiding me through it, as well. 

Furthermore, I wish to thank \emph{Kalle Kebbinau} for providing a constant
stream of valuable input and feedback on my work and ideas.

I'd also like to thank international Free and Open-Source Software community
for the immense pool of freely available components that made it possible
to learn and build anything imaginable.

\clearpage

\tableofcontents*

\mainmatter
\pagestyle{newchap}

\clearpage

%
%
% INTRODUCTION
%
%

\chapter{Introduction}
\label{chap:intro}

TODO

\section{Motivation}
\label{sec:mot}
The foundation of current work was established while author was setting up
the infrastructure to deploy Ubuntu 12.04 LTS on the PC-s of educational
institutions of Tallinn as part of the ongoing efforts of Tallinn Education
Department to switch from proprietary tools to open solutions
in order to avoid vendor lock-in.

Puppet was set up to manage Ubuntu workstations remotely. Local IT-support
took the role of bootstrapping the machines and joining them to remote
management server.

Customer A runs hundreds of embedded ARM computers for digital signage.
Software is currently updated by mailing the customer an SD-card with
updated software. The customer would prefer to update software and
media over the air but the software update atomicity has to be guaranteed
in order to avoid non-booting machines.

Customer B is about to deploy thousands of Ubuntu netbooks to be used as
remote workstations around the globe. It is vital to unroll security updates
as soon as possible, but at the same time it's necessary to guarantee
software update atomicity as the IT helpdesk is lacking in the remote
locations where the machines are used.
The solution has to be installable at customer premises and it
must make use of standard and recognized security methods.

Customer C has around thousand PC-s that need to be converted to Ubuntu,
but the budget is lacking and therefore manual labour has to be minimized.
Glitch-free software update mechanism is crucial part of minimizing manual
labour.

\section{Contributions}

The work produced a novel method of deploying and maintaining Linux
based workstations in an guaranteed and secure manner.

TODO

\section{Related Work}
\label{sec:related}

\subsection{Puppet and Foreman}

Puppet is a remote management system which features its own declarative
domain-specific language to describe the state of the configuration
\footnote{http://puppetlabs.com/}. Puppet server also known as Puppetmaster
hosts the configuration while managed machines run puppet agent which polls
the puppetmaster at specified interval, usually 30 minutes. Taken actions
are then reported back to the puppetmaster. Puppet agent and puppetmaster
both are written in Ruby and released the latest versions are released under
liberal Apache 2.0 license. Puppet can be used to manage both Linux and
Windows servers and workstations as well.

Puppet uses TLS based security model:
Puppetmaster maintains a certificate authority which is used
to sign certificate requests submitted by clients.
Fully qualified hostname is used to identify nodes and
it is also used derive common name for the X509 certificates.
Once a certificate is signed the nodes are expected to
execute whatever Puppetmaster dictates them to do.

Foreman is a complete lifecycle management software for physical and virtual
servers. Foreman incorporates Puppet, a custom web interface and provisioning
tools into single unified application. Even without using provisioning features
Foreman makes one of the most feature-complete web interfaces for Puppet
rivaling the Puppet Dashboard.

\subsection{Chef, Ansible and Salt}

Chef is infrastructure automation tool. Chef is written in Ruby and Erlang.
Chef uses domain-specific language written in Ruby.
Chef server stores your recipes as well as other configuration data.
The Chef client is installed on each server, virtual machine, container
or networking device or generally speaking node.
The client periodically polls Chef server latest policy and
state of the network. If anything on the node is out of date,
the client brings it up to date. 
\footnote{\url{https://www.chef.io/chef/}}.

Ansible remote management software uses SSH to connect to the nodes which
means there is no agent running on the managed machine, this however makes
it slightly more complicated to use Ansible to manage machines behind NAT.
Ansible is written in Python and it uses state-driven resource 
written in YAML.

Salt is an open-source configuration management system,
capable of maintaining remote nodes in defined states and 
a distributed remote execution system used to execute commands
on remote nodes, either individually or by arbitrary selection criteria
\footnote{\url{http://docs.saltstack.com/en/latest/topics/}}.
Salt is developed by SaltStack which sells services around Salt.
Salt uses ZeroMQ to transfer data between Salt server and
Salt minions.
Currently alternative transport
RAET (Reliable Asynchronous Event Transport)
is in development and it is designed Salt in mind.
RAET attempts to address more complex message routing
schemes which are not possible with ZeroMQ
\footnote{\url{http://docs.saltstack.com/en/latest/topics/transports/raet/index.html}}.


\subsection{Fully Automated Installation}


\gls{fai} is a non-interactive system to install, customize and manage
Linux systems and software configurations on computers as well as
virtual machines and chroot environments, from small networks to
large-scale infrastructures like clusters and cloud environments.
It's a tool for unattended mass deployment of Linux. The systems
are installed, and completely configured to your exact needs,
without any interaction necessary. 




\footnote{\url{http://fai-project.org/}}

\subsection{Clonezilla, Symantec Ghost, Acronis True Image}

Clonezilla \footnote{http://clonezilla.org/}
combines various open-source tools into a single cloning suite.
Clonezilla uses partclone utilities \footnote{http://partclone.org/} to
identify and transfer only used blocks of various filesystems, most notably
NTFS, ext4 and Btrfs.
Clonezilla supports resizing filesystems after the clone
in order to make use of the whole disk space available
in the target machine.
This makes it possible to use same prepared image for disks of
various size, the template image has to be of course smaller
than the target machine disks.
Clonezilla supports most Windows and Linux filesystems.

Symantec Ghost, previously known as Norton Ghost is a corresponding commericial
product currently available on the market
\footnote{http://www.symantec.com/ghost-solution-suite/}
offered by Symantec Corporation.
Acronis True Image
\footnote{http://www.acronis.com/en-eu/personal/pc-backup/}
is another similar product which supports Windows
and Mac OS X operating systems by Acronis International GmbH.

The fact that machines need to be taken offline is the main
drawback of classic disk cloning methods.

\subsection{FSArchiver}

FSArchiver
\footnote{http://www.fsarchiver.org/}
is a tool very much similar to Clonezilla,
but instead of storing disk image on a block level the
contents are stored on object-level (file, directory).
All filesystem attributes are preserved for Linux filesystems,
NTFS support is still experimental.
For archiving the filesystem has to be unmounted or mounted
read-only, with the assistance of LVM read-write mounted
filesystems can be snapshotted and archived afterwards.


\subsection{BSD Jails, Solaris Zones, OpenVZ, Linux Containers, systemd-nspawn}

Jails have been available in FreeBSD since version 4.x. Jails use chroot
syscall to substitute root filesystem of a process making it possible to
create a restricted environment which is isolated from the rest of the
operating system
\footnote{https://www.freebsd.org/doc/en/books/handbook/jails.html}.

Solaris Zones were introduced few years later adding similar
capabilities to Solaris operating system.
Solaris Zones took advantage of ZFS filesystem making it
possible to snapshot and clone zones.

Linux has included chroot for long time as it's essential feature for switching from initial root filesystem (initramfs/initrd) to actual root filesystem.
Many network services take advantage of chroot syscall to confine
itself to a particular directory in order to mitigate consequences
of vulnerabilities and exploits.

The main issue with chroot is that dependencies of the target
application have to be available in the chroot root filesystem.
For instance a Python application which has modules loaded before
chroot operation could operate without any files in the chroot,
but shell script which relies on several executables need to have
those utilities available in chroot as well.
With copy-on-write and de-duplicating filesystems such as Btrfs and
ZFS the problem how ever becomes irrelevant as root
filesystem of the chroots can be duplicated with no significant overhead.

\gls{lxc}
\footnote{\url{https://linuxcontainers.org/}}
takes advantage of the \emph{chroot} syscall and
recently Linux \emph{cgroups} (control groups subsystem) which permit
more  operating system level virtualization.
Control groups are used to implement limiting, accounting
and isolation of CPU, memory, disk I/O, network, etc resource usage.
LXC allows various backing stores, most notably ZFS, Btrfs and
OverlayFS which make it very easy to enable container
snapshotting and streaming backups.

\subsection{Docker and Rocket}

Docker started off as a way to automate container deployment and
configuration using containers and control groups present in Linux
kernel. As Docker started to add features that CoreOS developers
deemed excessive an alternative project Rocket was founded
\footnote{\url{http://www.theregister.co.uk/2014/12/03/coreos_rocket_deep_dive/}}.

\subsection{CoreOS and Ubuntu Core}

CoreOS \footnote{https://coreos.com/} is a rearchitected Linux
distribution which provides minimalist foundation to run containers.
It uses two-partition scheme to provide atomic updates of the root
filesystem. The operating system runs off a read-only filesystem
while the other one can be patched runtime. Reboot or \emph{kexec}
can be used to boot into the updated system. This prevents rendering
device unbootable due to interrupted upgrade.

Ubuntu Core is an Ubuntu flavour tailored towards Internet of Things
and as a container platform. Ubuntu Core introduced root filesystem
transactional updates to Ubuntu using Snappy
\footnote{http://developer.ubuntu.com/en/snappy/}.
Ubuntu Core is designed to run Docker applications and can be used.
Snappy is also plays important role in Ubuntu Phone ecosystem.
Snappy uses OverlayFS (?) to implement transactional updates.


\subsection{OverlayFS}

OverlayFS is a feature introduced in Linux 3.18 which makes it
possible to merge contents of two separate mountpoints on the fly.
OpenWrt uses OverlayFS to implement writable JFFS2 layer on top of
read-only SquashFS filesystem
\footnote{https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/Documentation/filesystems/overlayfs.txt}.

\subsection{LVM, mdadm and dmraid}

Ext4 has been primary filesystem for Linux based workstations and
servers for a while. It provides filesystem primitives such as files,
directories, permissions and timestamping.
In order to add redundancy
either software RAID or logical volume management (LVM) can be used.

Software RAID is implemented in Linux by means of \emph{mdadm}.
Software RAID can be used to build RAID1, RAID0, RAID10/01, RAID5
or RAID6 arrays without dedicated RAID controller which could also
impose a vendor lock-in.

LVM enables pooling of drives, mirroring and snapshotting by adding
an abstraction layer on top of physcal disks. Any filesystem that
can be deployed on physical disk can also be deployed on top of
LVM's logical volume. The kernel takes care of mapping logical
addresses to corresponding disk's physical address.
The snapshotting feature of LVM however has been claimed to be buggy
\footnote{http://lwn.net/Articles/522073/}.

\subsection{Btrfs and ZFS}

Btrfs and ZFS both are modern copy on write filesystems
which also fill in the role of volume manager.
Btrfs has been claimed to be unstable but the situation has
improved significantly over the past year or two.
Facebook has been testing Btrfs in production since the April of 2014
\footnote{\url{https://btrfs.wiki.kernel.org/index.php/Production_Users}}.
Chris Mason, a lead developer of Btrfs joined Facebook
in the end of 2013 with the goal of improving Btrfs support
for enterprise applications
\footnote{\url{http://article.gmane.org/gmane.comp.file-systems.btrfs/30420}}.

Btrfs supports redundancy in RAID0/1/10 configurations and
RAID5/6 support was added with Linux 3.19
\footnote{http://lkml.iu.edu/hypermail/linux/kernel/1412.1/03583.html}.
Btrfs supports zlib and lzo compression algorithms,
however enabling compression for Btrfs is known to
seriously hamper performance of database engines.

Btrfs supports subvolumes, which makes it possible to group
directories and files into logical units.
As mentioned above, LXC makes use of subvolumes
by confining the root filesystem to a Btrfs subvolume
if Btrfs backing store is used.
This means that root filesystem of a container
can easily be snapshotted.

In Btrfs context snapshotting is actually 
a generalization of subvolume cloning,
thus \emph{subvolume}, \emph{clone} and \emph{snapshot} may be used
interchangeably in certain contexts.
Due to copy-on-write architecture subsequent writes
to the original subvolume do not affect
the subvolume clone and there is no performance
degradation for the original subvolume.

Btrfs permits transmitting filesystem data over the network
by a concept of \emph{btrfs send} and \emph{btrfs receive}.
Btrfs send mandates the subvolume to be in a read-only mode,
which means that \emph{btrfs subvol snapshot} has to be issued
with extra \emph{-r} flag.
The Btrfs send bitstream contains filesystem operations that are
indended to be replayed on a clone of the parent subvolume:
create file, \emph{mkdir}, \emph{mknod}, \emph{mkfifo}, symlink,
\emph{link}, \emph{unlink}, \emph{rename}, \emph{rmdir}, open file,
close file, write to file, set/remove extended attributes,
truncate file, \emph{chmod}, \emph{chown}
\footnote{http://git.kernel.org/cgit/linux/kernel/git/kdave/btrfs-progs.git/tree/cmds-receive.c}.
During receive a subvolume is created 

Btrfs uses \emph{received\_uuid} and \emph{uuid} to
identify snapshots for transfer.
The \emph{received\_uuid} corresponds to the \emph{uuid} on
the initial machine and it remains intact for any subsequent transfers.
For differential snapshots few extra steps are taken.
During snapshot send an optimal parent snapshot is
identified and that is used as basis for the differential snapshot.
On the receiving end the a read-write snapshot of the parent
snapshot is created and filesystem operations
are replayed on the subvolume.
Finally corresponding \emph{received\_uuid} is set and
\emph{ioctl} is issued to set subvolume read-only.


\gls{grub}
\footnote{\url{https://bugzilla.redhat.com/show_bug.cgi?id=748071}}
is the main bootloader used
by Linux-based operating systems on x86 and PowerPC based machines.
Earlier versions of GRUB supported multi-stage booting process,
which meant that GRUB stage1 binary was embedded in the
master boot record which loaded stage1.5 embedded
32256 byte area between the MBR and first partition.
The stage1.5 contained filesystem drivers which could
address the filesystem contents and boot stage2 from
the /boot/grub of the Linux filesystem.

GRUB2 dropped support for multi-stage booting process,
instead it is recommends that the first partition
starts at megabyte boundary, leaving
more than 500kB room between MBR
and first partition which is well enough to accommodate
feature-rich bootloader.
GRUB2 added support for booting from Btrfs root filesystem
in various configurations
\footnote{\url{https://btrfs.wiki.kernel.org/index.php/FAQ\#Does_grub_support_btrfs.3F}},
hence in order to boot from
Btrfs a GRUB2 installation is required and older versions of
GRUB are not supported.
GRUB2 also now supports booting from a particular subvolume,
making it possible to place several root filesystems
in same Btrfs pool.

Debian has supported Btrfs since Squeeze and has improved support
since then \footnote{https://wiki.debian.org/Btrfs}.
Ubuntu also has supported Btrfs for a while with customized
subvolume naming scheme:
root filesystem is placed in subvolume named \emph{@}
and home directories in a subvolume called \emph{@home}.

\subsection{apt-btrfs-snapshot and yum-fs-snapshot}

In Ubuntu package repositories there is available
\emph{apt-btrfs-snapshot}
\footnote{https://launchpad.net/apt-btrfs-snapshot}
package,
which creates snapshot of the root filesystem subvolume \emph{@}
before every \emph{apt-get} operation.

\emph{yum-plugin-fs-snapshot}
\footnote{\url{http://man7.org/linux/man-pages/man1/yum-fs-snapshot.1.html}}
is the corresponding package for Fedora and
Red Hat based distributions.

These approaches make it possible to boot into previous snapshots
in case there are issues with the updated packages.

\subsection{FreeNAS, Rockstor and OpenMediaVault}

FreeNAS is a FreeBSD \footnote{http://www.freenas.org/}
based distribution which builds a complete NAS solution on top of
ZFS filesystem and web interface.
Rockstor \footnote{http://rockstor.com/}
is a complementary CentOS based solution that uses Btrfs instead
of ZFS filesystem.
OpenMediaVault provides similar functionality using Debian instead of CentOS
\footnote{http://www.openmediavault.org/}.
All three of them support SMB/CIFS (Windows file shares),
NFS (UNIX file shares) and filesystem aided snapshots.





\subsection{rsync and rsnapshot}

rsync is an open source utility designed for
fast incremental file transfer.
It is most commonly invoked with archiving flag (-a) which
retains permissions, ownership, timestamps and symlinks.
In that case files are transferred only if
modified timestamp or file length differs.

rsnapshot is an utility that takes advantage of hardlinking
functionality of ext4 and other similar filesystems.
By creating a clone of a directory tree using hardlinks no
extra disk space is consumed. Applying classical rsync on top of
the clone only increments by the disk usage of changed files.


\subsection{OpenWrt and DD-WRT}

Most Linux based open-source router firmwares originate from
Linksys WRT54G wireless router.
Initially Cisco did not provide source code for the router as GPL mandates.
Between 2003-2008 Free Software Foundation attempted to cooperate
with Cisco to work out issues, but as Cisco continued to release
new devices with similar issues Free Software Foundation eventually
sued Cisco for malpractice.
After the lawsuit Cisco complied and has been releasing firmware
sources for all of their devices which make use of software
released under GPL licenses.
\footnote{http://www.fsf.org/licensing/2008-12-cisco-complaint}.
Linksys WRT54G sources were basis for various Linux distributions for routers such as
DD-WRT \footnote{http://www.dd-wrt.com/},
Tomato \footnote{http://www.polarcloud.com/tomato}
OpenWrt \footnote{https://openwrt.org/}.

OpenWrt as we know it today is a Linux distribution for embedded devices
which attempts to provide writable filesystem and package management.
OpenWrt supported hardware list mainly targets routers, but other devices are
listed as well \footnote{http://wiki.openwrt.org/toh/}. OpenWrt can be used to
extend lifetime of equipment that otherwise would be largely obsolete due
to unmaintained software from hardware manufacturer.

Most high-end consumer grade routeres employ 8MB NAND Flash chip which is
directly connected to the SoC without controller
\footnote{http://wiki.openwrt.org/doc/techref/flash.layout}.
The Flash storage is usually partitioned at least as 3 slices:
bootloader, read-only root filesystem, read-write overlay.

The read-only root filesystem contains SquashFS
\footnote{http://squashfs.sourceforge.net/}
which is highly-efficient compressed read-only filesystem that
supports variety of compression algorithms.
The read-write overlay partition
is formatted as JFFS2 (journalling flash filesystem).

This method makes it possible to: perform factory reset simply by
formatting the JFFS2 partition and upgrading firmware by overwriting
SquashFS partition. Due to lack of redundancy in consumer-grade routers
an interrupted firmware upgrade usually renders device unfit for use.
This is also known as \emph{bricking} in embedded developer jargon.


\subsection{Android}

Android ROM images are typically distributed as zip files which contain 
binary blobs for modem and bootloader in addition to snapshots of the
file primary filesystems of Android: boot, cache, recovery, system and
userdata
\footnote{https://dl.google.com/dl/android/aosp/shamu-lrx22c-factory-ff173fc6.tgz}.
Differential images are also available, in that case zip file contains
directory tree of files intended to be overwritten or added to the original
root filesystem and post-installation scripts which correct the file permissions
\footnote{http://gapps.itvends.com/gapps-lp-20141212-signed.zip}.

ROM manager such as ClockworkMod \footnote{https://www.clockworkmod.com/} or
TWRP \footnote{http://teamw.in/project/twrp2} has to be used to install or patch
third-party ROM-s.
An alternative Fastboot method is also present in most
Android devices and it can be used to directly write raw filesystem images and
unlock the device
\footnote{\url{http://wiki.cyanogenmod.org/w/Doc:_fastboot_intro}}.
As Android is open-source vendors tend to customize various aspects
of the software update process.




\subsection{OpenStack}

OpenStack is a free and open-source cloud computing software platform
which is composed of several components of which most noteworthy for current work are:
Glance image service, Ironic bare metal provisioning,
Swift object storage and Cinder block storage.
Glance provides discovery, registery and retrieval services of virtual machine images
\footnote{http://docs.openstack.org/developer/glance/}.
Glance BitTorrent delivery enables BitTorrent support for transferring the images
\footnote{https://blueprints.launchpad.net/glance/+spec/glance-bittorrent-delivery}.





%
%
% BACKGROUND
%
%
\chapter{Background}
\label{chap:bgr}


\section{Initial task}

The initial task was to use Ubuntu as operating system basis
for schools due to rich set of both open-source and
proprietary software components available in Ubuntu ecosystem.
So far the machine deployment was a tedious task involving
manual labour:
The Ubuntu 12.04 LTS image had to be downloaded from the Internet
and transferred to a memory stick.
The Ubuntu installer was booted from the memory stick
and usual installation was performed which took roughly 20 minutes.
The machine was booted into Ubuntu,
Puppet was installed on the machine and Puppet configuration
was tweaked to use our server.
This took another 10 minutes and was not a procedure that could
be performed by a novice user using (pseudo-)graphical user interface.
Once the certificates was signed on the Puppetmaster the
machine downloaded necessary packages and applied configuration
changes.
Usually this would take several Puppet runs and as a result
setting up a classroom of computers took several days
depending on the command-line proficiency of local
IT-support, which in some cases was close to zero.

\section{Problems with package management}

Ubuntu uses APT (Advanced Packaging Tool) as basis for
it's package management.
APT was originally developed as part of Debian operating system
to be used as \emph{dpkg} frontend.
While \emph{dpkg} can be used to install and remove packages,
it does not provide dependency tracking nor fetching
packages from remote locations which are implemented by APT.
APT significantly simplifies the installation of software
components by downloading packages from different sources
and checking package dependencies prior installation.

Ubuntu Software Center builds another abstraction on top of APT,
while hiding libraries and other system components it enables
even more simplified installation of apps for Ubuntu based
machines, for remotely managed machines the Ubuntu Software Center
and other graphical package management tools were removed.

\begin{figure}[tbhp]
  \centering
  \includegraphics[width=\textwidth]{images/ubuntu-software-center.png}
  \caption{Ubuntu software center}
  \label{fig:ubuntu-software-center}
\end{figure}

There are however certain corner-cases where APT may render
the package management unusable.
For instance package list corruption was faced on several occasions,
in that case APT crashes with segmentation fault
\footnote{http://askubuntu.com/questions/532200/14-04-lts-apt-get-segfault}
and currently the only known solution to the problem
involves deleting package lists and running \emph{apt-get update} again.
Several faulty packaging scripts were stumbled on,
for example it was not possible to remove certain versions of LibreOffice packages
and manual intervention was necessary
\footnote{\url{https://www.linuxquestions.org/questions/showthread.php?s=e0e2f7689f847a56e8cee94a0cafd6bd&p=5216367\#post5216367}}.

Debian community has been working hard to provide differential updates for
the packages, but as of February 2015 the efforts have proven fruitless.
Differential updates are applied for package lists
\footnote{\url{https://www.debian-administration.org/article/439/Avoiding_slow_package_updates_with_package_diffs}},
but binary diffs for packages have not implemented yet.
Fedora community has however successfully deployed differential packages
\footnote{http://fedoraproject.org/wiki/Features/Presto},
thus reducing the amount of data needed to be transferred during an package update. For bigger software (eg LibreOffice) the lack of differential updates poses a serious concern, especially for low-bandwidth links.
Puppet, SaltStack, Chef, Ansible and other traditional configuration
management fit best the scenario where each node has slightly different
configuration and it makes sense to keep them separate. However
provisioning very similar nodes with for instance Foreman has obvious
overhead - each node has to fetch updated packages independently from
the same APT repositories, same has to be done for application software.

Release upgrades for example from Ubuntu 12.04 to Ubuntu 14.04
have proven to be especially troublesome due to the fact that system libraries
and files are updated and interrupted release upgrade may leave system
in an unusable state.

As it has hopefully become clear by now
installation of software for Ubuntu and Debian
is most usually performed using APT in one form or another.
Software that is not available in an APT repository
is troublesome to install.
For instance Smarttech distributes software for their smart
whiteboard products as a .zip of Debian packages.
Similarily Canon printer drivers are available as a .zip file.
Last version of Acrobat Reader is also not available
from any APT repository for Ubuntu 14.04.
Skype distributes a Debian package for Ubuntu, but again
not from a APT repository.
Setting up an APT repository is not a trivial task,
even for an experienced Linux sysadmin.


For classroom deployment cloning has been used in the past:
Windows, Ubuntu or both are installed on a physical template machine.
Template machine is thoroughly tested.
Tools such as Clonezilla or Symantec Ghost are used to transfer the
harddisk image to the other machines.
As the whole procedure is a complex undertaking
it is usually performed once a year in summer especially for
educational institutions.
In fact cloning was used by some of the participating
schools - for example Alan Õis, the IT-support at Mustamäe Gümnaasium
used Clonezilla to set up his classrooms.



%\begin{figure}[!htb]
%\centering
%\scalebox{0.5}{\input{dia/traditional-partitioning.tex}}
%\caption{Partitioning with separate /home}
%\label{fig:traditional-partitioning}
%\end{figure}

%Even so separate filesystem for home directories
%as described in Figure~\ref{fig:traditional-partitioning} has
%proven to be effective method against wiping the whole disk
%during reinstall of Ubuntu.

\clearpage

\section{Specification}

Considering the needs of the commercial customers and
experience gained in the first iteration of the migration project
following list of requirements were specified for next iteration.

\begin{enumerate}
\item The solution has to support all major Linux distributions - Ubuntu, Fedora, Red Hat, etc.
\item The software upgrades have to be atomic, in other words interrupted updates can not render a workstation unusable. 
\item Software upgrades must retain domain join without having to join machine to a domain.
\item The home directories must remain intact during software updates.
\end{enumerate}

\noindent Following requirements were specified for the provisioning stage.

\begin{enumerate}
\item \gls{pxe} support
\item Bootable off USB memory stick
\item Bootable off CD-R
\end{enumerate}

\noindent Following requirements were specified for the server:

\begin{enumerate}
\item It has to be possible to run a central server for all nodes
\item It has to be possible to run a local instance
\item Push and pull capabilities to transfer templates between servers
\end{enumerate}

\noindent Following requirements were specified for security:

\begin{enumerate}
\item As initial provisioning can be assumed to be done on premises,
man-in-the-middle attacks can be ruled out in that
provisioning stage.
\item It has to be possible to verify subsequent incremental snapshots by means of asymmetric keys.
\item Consistent methods for root filesystem template fingerprinting have to be provided
\end{enumerate}


%
%
% DESIGN
%
%
\chapter{Butterknife Design and Architecture}
\label{chap:design}
We have outlined a number of existing methods and tools 
which address various aspect of Linux deployment on workstations
in chapter \ref{chap:intro}, and it has hopefully become clear
the functionality and guarantees provided by the presented systems.
However, it is clear that there will never be a single tool
that is optimal for every application. 

Yet, many problems are a shared concern between all of these systems.
Foremost among these are \emph{bootstrapping}, i.e. getting
the initial software setup on the machine,
\emph{upgrading}, that is updating the software components on the machine and
\emph{configuring} the software components.

To address this issue, we present \emph{Butterknife},
a provisioning suite that provides solutions for bootstrapping and
upgrading Linux-based workstations, while remaining flexible enough
to be used to deploy any Linux-based operating system
and to be used in conjunction with already existing
configuration management tools such as Puppet and Salt.

\clearpage

\section{Concepts}
\label{sec:concepts}

The prototype draws inspiration from embedded computers
where certain guarantees have to be provided.
The workflow for the prototype is described in
Figure~\ref{fig:butterknife-workflow}.\\

\begin{figure}[!htb]
\centering
\scalebox{0.6}{\input{dia/workflow.tex}}
\caption{Workflow for Btrfs snapshot based software deployment}
\label{fig:butterknife-workflow}
\end{figure}

\noindent The prototype consists of four major components:

\begin{enumerate}
\item Helpers to build a template.
\item Python based HTTP API to serve the snapshots.
\item Buildroot based provisioning image.
\item DBus services for applying incremental snapshots online.
\end{enumerate}

\clearpage

\section{Template helpers}

LXC containers are used to bootstrap the template for provisioning.
Creating container with Btrfs backing store (lxc-create -B btrfs)
on top of a Btrfs filesystem places the container in an
isolated subvolume which makes it easy to snapshot the container.
Within the container \emph{puppet apply} and similar methods can be used
to take advantage of already existing configuration management know-how.
Otherwise traditional manual labour can be employed to set up the template: installing packages, tweaking configuration files etc.

During the release phase the LXC container is stopped, pre-release
scripts are executed to clean up package cache and temporary files.
Then a read-only Btrfs snapshot is generated from the container root filesystem.
At this point new snapshot becomes available via HTTP \gls{api}.


\clearpage

\section{HTTP API}

Falcon HTTP API from Rackspace was used to build the
API for Butterknife server
\footnote{\url{http://falconframework.org/}}.
The HTTP API serves information about templates
available in the server's \emph{/var/butterknife/pool}.
The API exposes several methods:

\begin{enumerate}
\item \emph{/api/template/}
\item \emph{/api/template/\{name\}/arch/\{arch\}/version/}
\item \emph{/api/template/\{name\}/arch/\{arch\}/version/\{version\}/stream/}
\end{enumerate}

The \emph{name} of a template follows naming scheme of DBus objects
incorporating fully qualified domain name in reverse and the identifier
of the object.
The \emph{version} refers to the snapshot of the template.
The \emph{arch} refers to target architecture which is normalized
to \emph{x86} for 32-bit and \emph{x86\_64} for 64-bit Intel x86 machines.
For example the Btrfs subvolume stream URL for
snap42 of the 32-bit EduWorkstation that originates from butterknife.koodur.com would be

\emph{/api/template/com.koodur.butterknife.EduWorkstation/arch/x86/version/snap42/stream}.

Note that the stream URL also accepts parent argument,
so incremental snapshot can be received simply by appending
\emph{?parent=snap41}.
The unicast snapshot transfer topology is
shown in Figure~\ref{fig:butterknife-usecase-http}.
Note that unicast suffers obvious scalability issue,
the uplink of the server is eventually congested and
throughput per node decreases with every additional node.

\begin{figure}[!htb]
\centering
\scalebox{0.5}{\input{dia/butterknife-usecase-http.tex}}
\caption{Deployment over HTTP}
\label{fig:butterknife-usecase-http}
\end{figure}

Multicast is used to resolve the scalability issue of
the initial provisioning stage.
Snapshot transfer topology with off-site
server is shown in Figure~\ref{fig:butterknife-usecase-multicast}.
In this case the snapshot is transferred over HTTP from
the server by one of the participating nodes.
That node proxies the stream to local LAN segment using multicast.
All the other nodes are receiving over multicast

\begin{figure}[!htb]
\centering
\scalebox{0.5}{\input{dia/butterknife-usecase-multicast.tex}}
\caption{Deployment over multicast}
\label{fig:butterknife-usecase-multicast}
\end{figure}


\clearpage

\section{Provisioning image}

Using Debian, Ubuntu and Gentoo were evaluated as provisioning utility
operating system. With Debian and Ubuntu the resulting PXE bootable image
would have exceeded 100MB.
As of February of 2015 the CoreOS image suffers similar issue -
\emph{vmlinuz}
\footnote{\url{http://stable.release.core-os.net/amd64-usr/current/coreos_production_pxe.vmlinuz}}
and
\emph{initrd}
\footnote{\url{http://stable.release.core-os.net/amd64-usr/current/coreos_production_pxe_image.cpio.gz}}
files required to boot over PXE are correspondingly 24MB and 117MB.
With Gentoo significant tweaking is required, because Gentoo is
mainly targeted for power users.

Using Python to build pseudo-graphic menu-driven user interface was
evaluated and deemed not necessary for the goal as Python runtime and
dependant libraries add about 10MB to the resulting image.
In addition to that \texttt{parted} Python bindings were unavailable
in Buildroot package selection.

Buildroot was eventually used to generate an compact 10MB all-in-one
PXE-bootable image. Utilities \texttt{dialog} in conjunction with
\texttt{curl}, \texttt{jq} and others were used to build the user-interface
and Bash was used to program the user-interface logic.
Resizing of NTFS filesystems is provided by \texttt{ntrfsresize} utility
which is part of \texttt{ntfs-3g} package, this eases deployment of
dual-boot machines.
Complete multicast is supported via consistent snapshot naming scheme
and \texttt{udpcast-receive} and \texttt{udpcast-sender} utilities which
are part of \texttt{udpcast} package.

The security model for the initial deployment phase could be improved
as only method of verification of the source is the certificate
authority chain verified by \emph{curl} during the Btrfs snapshot
retrieval.

The used partitioning scheme is described in Figure~\ref{fig:pooled-partitioning}
is inspired by Ubuntu and Lennart Poettering's article.
The whole block device is allocated to single Btrfs filesystem
which is mounted at \texttt{/var/butterknife/pool} making
it possible to easily iterate over templates and root filesystem instances
present in the machine.
Whenever a Btrfs (differential) snapshot stream is received
the incoming subvolume is placed under \texttt{/var/butterknife/pool}.
Before transfer remnants of previously interrupted transfers are cleaned
up by simply iterating over directories beginning with \texttt{@template:}
and attempting to create a file inside the directory.
If the file creation succeeds, the subvolume is deleted.
Note that prior successful exit \texttt{btrfs receive} sets the
subvolume read-only, thus file creation should fail.

Once the template subvolume is successfully received
a read-write clone is made with \texttt{@root:} prefix,
this signifies an \emph{instance} of a root filesystem.
The instance subvolume is mounted at \texttt{/mnt/target}
and several mountpoints such as \texttt{/dev/},
\texttt{/proc/}, \texttt{/sys/} are bound to \texttt{/mnt/target}.

Finally \texttt{chroot} is issued to enter the instance to
run \texttt{butterknife-postdeploy} which executes
post-deploy scripts which are part of the template.
Most usually the Btrfs pool is mounted at \texttt{/var/butterknife/pool}
of the instance and \texttt{@persistent} subvolume is created
in the pool.
The persistent subvolume is then mounted at \texttt{/var/butterknife/persistent}.
The persistent subvolume is used to retain hostname,
domain join information (Kerberos keytab, Samba secrets),
Puppet keys and certificates etc.
Final aspect of the postdeploy scripts is the 
bootloader installation.
As the root filesystem is esentially swapped out,
\texttt{grub-install} is required to update references
to the newly created OS instance.

\begin{figure}[!htb]
\centering
\scalebox{0.5}{\input{dia/pooled-partitioning.tex}}
\caption{Pooled partitioning}
\label{fig:pooled-partitioning}
\end{figure}


\section{DBus services}

For running nodes a DBus service was written to poll the snapshot server
for updates and another DBus service was written in Python to notify user
about available updates.



%
%
% EVALUATION
%
%
\chapter{Experimental Evaluation}
\label{chap:eval}
In order to show that our basic approach in Butterknife is practical and
and extendible for large-scale applications such as unrolling
updates for smartphones or other embedded devices.

TODO

Using Butterknife it is easy to bundle proprietary components
into the template, thus setting up APT repository to distribute
packages for the workstation is not necessary.

\clearpage

\section{Benchmarks}

The resulting full snapshot for schools was based on 32-bit Ubuntu 14.04 and
it included MATE 1.8.2 desktop environment, Mozilla Firefox 37 web browser,
LibreOffice 4.4.2 office suite,
VLC 2.1.6 multimedia player,
Skype 4.3.0 voice over IP solution,
Inkscape 0.48.4 scalable vector graphics editor,
Gimp 2.8.10 photo editor and many tweaks
to make the system usable in Estonian schools.

The throughput of the server-client channel depends on
several factors: 
raw snapshot bitstream size,
network bandwidth;
compression algorithm;
parallelization of compression;
encryption;
storage medium.

The disk usage of resulting
\texttt{@template:com.koodur.butterknife.EduWorkstation:x86:snap83}
subvolume was approximately 6.20GiB
due to fragmentation and holes in sparse files.
Issuing \texttt{btrfs send} on the subvolume
results in approximately 5.75GiB bitstream.
Compressing the bitstream yields different
results depending on the compression algorithm:\\

\begin{tikzpicture}
    \begin{axis}[xbar,width=12cm, height=7cm,
        xmin=0,xlabel={Resulting file size (GiB)},
        symbolic y coords={xz, bzip2, gzip, lz4 -9, lz4 -1, raw},
        nodes near coords, nodes near coords align={horizontal},
        ytick=data]
        \addplot coordinates {
            (5.75,raw)
            (3.21,lz4 -1)
            (2.77,lz4 -9)
            (2.51,gzip)
            (2.32,bzip2)
            (1.87,xz)
        };
    \end{axis}
\end{tikzpicture}

Note that streaming compression is necessary for scenarios
where differential snapshots are not stored as static files
or where the full snapshots are directly served from
\texttt{btrfs send} which is also the default case for Butterknife.
Direct streaming with \texttt{btrfs send} makes it possible
to take full advantage of the copy-on-write filesystem
and snapshots while keeping disk usage on the server side minimal.

Compression time was measured on Intel(R) Core(TM) i7-4770R CPU 
clocked at 3.20GHz with 2x 8GB DDR3 memory modules running
Ubuntu 14.04 LTS and 3.16.0 kernel.
The source and destination directories were
mounted as \texttt{tmpfs} on the same 
machine to exclude the
HDD/SDD and network effects on the test.
The parallel version of \texttt{gzip} algorithm,
\texttt{pigz} is included in the results as well as \texttt{pxz}
the parallel version of \texttt{xz}.
\\

\begin{tikzpicture}
    \begin{axis}[xbar,width=12cm, height=6cm,
        xmin=0,xlabel={Real time (sec)},
        symbolic y coords={xz, bzip2, pxz, gzip, pigz},
        nodes near coords, nodes near coords align={horizontal},
        ytick=data]
        \addplot coordinates {
            (46,pigz)
            (206,gzip)
            (494,pxz)
            (555,bzip2)
            (2150,xz)
        };
    \end{axis}
\end{tikzpicture}

Note that \texttt{pxz} actually used 53m41.214s of CPU time
and \texttt{pigz} used 5m31.287s of CPU time.



As root filesystem contains numerous small files significant
slowdown was imminent if spinning disk was used on the either side.
An average throughput of ~37MB/s was observed while
running \texttt{btrfs send} from or \texttt{btrfs receive} to
Western Digital WD40EFRX without using encryption or compression
over plain HTTP.
Using arbitrary SSD-s on both ends averaged around 100MB/s
due to gigabit ethernet used for benchmarking.


\begin{tikzpicture}
    \begin{axis}[xbar,width=8cm, height=12cm,
        xmin=0,xlabel={Average throughput (MB/s)},
        symbolic y coords={xz on i7-4770R, Fast Ethernet, bzip2 on i7-4770R, pxz on i7-4770R, gzip on i7-4770R, btrfs receive on Samsung M9T, btrfs send on WD40EFRX, btrfs send on SDSSDHP256G, Gigabit Ethernet, btrfs send on SSDMCEAW240A4, pigz on i7-4770R},
        nodes near coords, nodes near coords align={horizontal},
        ytick=data]
        \addplot coordinates {
            (125,pigz on i7-4770R)
            (112,btrfs send on SSDMCEAW240A4)
            (100,Gigabit Ethernet)
            (86.2,btrfs send on SDSSDHP256G)
            (58,btrfs send on WD40EFRX)
            (27,btrfs receive on Samsung M9T)
            (27,gzip on i7-4770R)
            (11.6,pxz on i7-4770R)
            (10.3,bzip2 on i7-4770R)
            (10,Fast Ethernet)
            
            (2.6,xz on i7-4770R)
        };
        
    \end{axis}
\end{tikzpicture}

The empiric observations carried out with
the snap83 of the root filesystem  pointed out
several aspects which went previously unnoticed.
The most important conclusion that can be drawn from the 
results is that enabling compression may actually cap the
deployment speed.
If single-threaded \texttt{gzip}, \texttt{pxz} or
\texttt{bzip2} might make sense for 100MBps infrastructure,
then going beyond gigabit threshold the
law of diminishing returns can be observed.
Additionally the harddisks with spinning platters
as well as solid-state disks impose throughput
limitations due to prolonged seek times
caused by normal filesystem fragmentation. 

The compression aspect is notable because
Estonian Educational Network has scheduled infrastructure
upgrades for 2016 and their plan is to supply
gigabit link to every educational institution of Estonia.


%
%
% CONCLUSIONS
%
%
\chapter{Conclusions and Future Work}
\label{chap:conc}

TODO



\section{Conclusions}

The implementation satisfies, exceeds to be precise,
the requirements of educational institutions.
It significantly decreases local IT-support work,
by reducing workstation bootstrap down to 15 minutes even for
a regular sized classroom.

Source code of the solution was published at GitHub
\footnote{https://github.com/v6sa/butterknife}.
The instructions for setting up similar infrastructure
are provided at GitHub and are constantly being improved,
making it possible for virtually anyone
to roll a customized distribution
for particular purpose.

Butterknife also omits the overhead
and complexity associated with traditional
ISO remastering systems such as \emph{remastersys}
\footnote{\url{https://help.ubuntu.com/community/LiveCDCustomization}}.


\section{Push and pull capabilities}

As of kernel 3.17 there is no consistent way of transferring
differential snapshots in a distributed Git-like fashion due to way
\texttt{btrfs receive} locates the parent subvolume.
Whenever \texttt{btrfs send} is issued, the \texttt{uuid} of origin
subvolume is bundled with the bitstream, that becomes
the \texttt{received\_uuid} on the receiving endpoint and
new \texttt{uuid} is assigned for the created subvolume.
For incremental snapshots \texttt{btrfs send} also bundles
the \texttt{uuid} of the parent subvolume,
now \texttt{btrfs receive} attempts to locate the parent
subvolume by \texttt{received\_uuid} of local subvolumes.

This effectively restricts the workflow to one direction
as shown in Figure~\ref{fig:btrfs-received-uuid-issue}.
This prevents running a downstream Butterknife server and
even more importantly it makes tricky restoring snapshot
in scenarios where Btrfs is used for backuping.


\begin{figure}[!htb]
\centering
\scalebox{0.5}{\input{dia/btrfs-received-uuid-issue.tex}}
\caption{UUID inconsistency}
\label{fig:btrfs-received-uuid-issue}
\end{figure}

Arne Jansen, previously a Btrfs developer maintains far-progs repository
\footnote{\url{https://www.kernel.org/pub//scm/linux/kernel/git/arne/far-progs.git}},
which contains tools for manipulating Btrfs snapshot streams,
this format is also known as FAR (Filesystem ARchive).
The repository contains most notably three utilities
\texttt{fssum}, \texttt{fardump} and modified version of \texttt{btrfs-receive}.
The first one, \texttt{fssum} can be used to calculate checksum of a
directory tree, making it possible to verify received snapshot,
\texttt{fssum} also has flags to omit modification times and other information that 
in certain context may be interpreted as noise.
The second one \texttt{fardump} can be used to examine FAR stream contents
in a human readable format.
Finally the \texttt{btrfs-receive} from \texttt{far-progs} features
two extra flags: \texttt{-p} which disables
automatic parent searching and allows user to specify which 
subvolume to use as a parent and \texttt{-d} which 
allows specifying a custom subvolume name.

Disabling parent search makes it possible to detach
Btrfs' UUID mechanism and customize the differential snapshot logic.
In this case care must be taken of snapshot consistency,
applying differential snapshot on incorrect parent subvolume
may cause data corruption or end up with a crash.

\section{Adding BitTorrent transport}

Currently only HTTP(S), multicast and their combinations are supported
for initial provisioning stage.
Multicast makes sense for scenarios where there is simultaneous
control over multiple machines.
OpenSSH is used for transferring snapshots between servers.
The differential snapshots are primarily downloaded
via HTTP(S) because coordinating multicast
transfers with roaming laptops is a complex scenario.

As getting consistent output from \texttt{btrfs send}
is tricky it makes sense to store the incremental snapshot
bitstreams as files and use BitTorrent to redistribute them.
This way \texttt{received\_uuid} can be kept in sync with the origin


BitTorrent is a protocol designed for \gls{p2p} file transfer.

The incremental snapshots could be transferred
using BitTorrent, distributing the load among
nodes.

\begin{figure}[!htb]
\centering
\scalebox{0.5}{\input{dia/butterknife-usecase-bittorrent.tex}}
\caption{Possible load distribution scenario using BitTorrent}
\label{fig:butterknife-usecase-bittorrent}
\end{figure}




\section{Deep BitTorrent integration}

A .torrent file contains metadata about the content in question and a
tracker URL - that is the service which is used to discover other peers in the
pool of participating nodes also known as swarm.
BitTorrent splits files into pieces and SHA-1 hash is calculated per piece.
BitTorrent protocol does not specify minimal piece length
\footnote{\url{http://www.bittorrent.org/beps/bep_0003.html\#info-dictionary}},
but for example libtorrent imposes restriction of having piece length
a multiple of 16KB
\footnote{\url{http://www.libtorrent.org/reference-Create_Torrents.html\#id5}}.
It is reccommended to keep BitTorrent file size below 100kB,
which means the piece size is correlated to content size
\footnote{\url{https://wiki.vuze.com/w/Torrent_Piece_Size}}.

For multi-file torrents the files in the directory tree are handled
as a continuous stream of data of the concatenated files
as shown in Figure~\ref{fig:torrent-multifile},
thus changing size of a file that happens to be placed in the beginning of a torrent
file results in completely different checksums for the whole torrent.
Such approach is reasonable for rarely changing data, but for current usecase
causes significant overhead.


\begin{figure}[!htb]
\centering
\scalebox{0.35}{\input{dia/torrent-multifile.tex}}
\caption{Multi-file torrent handles directory tree as a continous stream of data}
\label{fig:torrent-multifile}
\end{figure}

BitComet has implemented \emph{Align File to Piece Boundary} function,
which adds a padding file if necessary to align files to piece boundary
as shown in Figure~\ref{fig:torrent-multifile-aligned}.
This way pieces which contain identical files retain same piece checksums,
however small files add significant overhead due to padding files.
Compression of pieces sent on the wire has been proposed
\footnote{\url{https://wiki.theory.org/BitTorrentWishList}}.
This could resolve the overhead issue introduced due to padding files.
The problem of piece alignment could also be addressed
simply by introducing new file mode.

\begin{figure}[!htb]
\centering
\scalebox{0.35}{\input{dia/torrent-multifile-aligned.tex}}
\caption{Align File to Piece Boundary}
\label{fig:torrent-multifile-aligned}
\end{figure}


Btrfs uses crc32c for checksumming and support for additional
checksum algorithms, namely SHA1 is planned
\footnote{\url{https://btrfs.wiki.kernel.org/index.php/Project_ideas\#More_checksumming_algorithms}}.
Current on-disk format supports up to 256-bit hash checksum per
metadata block and arbitrary count of hashes for per-block checksums.
\footnote{\url{https://btrfs.wiki.kernel.org/index.php/FAQ\#What_checksum_function_does_Btrfs_use.3F}}
As of November 2013 Btrfs defaults to 16kB or page size
whichever is larger.
On most Linux workstation page size is set to 4kB, thus 16kB block
size takes precedence.
\footnote{\url{https://git.kernel.org/cgit/linux/kernel/git/mason/btrfs-progs.git/commit/?id=c652e4efb8e2dd76ef1627d8cd649c6af5905902}}.

As online deduplication is in works for Btrfs it makes sense to
combine the two, implementing additional \texttt{ioctl} for Btrfs in
order to perform block lookup by checksum is trivial task.
This could make it possible to implement high-performance
BitTorrent implementation which takes advantage of Btrfs metadata.

Aligned checksumming would permit sharing platform-independent
(images; fontconfig cache; dconf database; LaTeX packages;
Bash, Python, Ruby, Perl, Lua, Java source and bytecode)
file chunks between machines of different architecture (amd64, i386, armel, armhf)
from arbitrary snapshots.

Generating .torrent corresponding to a root filesystem
has other issues as well.
Piece size of 16kB results in a torrent file
exceeding 10MB for Ubuntu root filesystem and that didn't even
include padding files required to align piece boundary to file beginning.
Most BitTorrent client implementations fail to handle .torrent
files of such size resulting in out of memory errors or freezes.
Also BitTorrent currently does not handle symlinks,
POSIX filesystem permissions and access control lists
\footnote{\url{http://users.suse.com/~agruen/acl/linux-acls/online/}}.

This leads us to believe that these properties be transferred
using additional manifest file, making use of .torrent file redundant.

\section{Alternative filesystem layouts}

Lennart Poettering, an controversial Free Software developer has outlined a method
\footnote{http://0pointer.net/blog/revisiting-how-we-put-together-linux-systems.html}
of building Linux based systems using Btrfs snapshots.
The new layout requires significant effort from operating system distributors,
software suite vendors but promises significant save of effort on testing
software on Linux based systems.
Most notably a snapshot naming scheme is proposed in the article,
which would permit mixing operating system files with different
sets of libraries, frameworks and applications.


%
%
% APPENDIX
%
%

\appendix


\clearpage
\twocolumn
\printglossaries
\clearpage
\onecolumn

\end{document}
